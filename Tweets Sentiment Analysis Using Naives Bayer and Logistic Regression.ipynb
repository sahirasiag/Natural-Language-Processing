{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                  # Python library for NLP\n",
    "from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re                                  \n",
    "import string   \n",
    "import random\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use for this project is the sample twitter dataset from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the lists of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPositive Tweet:\n",
      "For most of you it is #GoodMorning but for me it is #GoodNight...\n",
      "#sleeptight for me and #haveagoodday for you!  :)\n",
      "\u001b[91mNegative Tweet:\n",
      "@WforWoman \n",
      "A9.\n",
      "It would be Ice Cream without Ice :((\n",
      "#WSaleLove\n"
     ]
    }
   ],
   "source": [
    "# example of positive and negative tweets\n",
    "print('\\033[92m' + \"Positive Tweet:\\n\" + all_positive_tweets[random.randint(0,5000)])\n",
    "print('\\033[91m' + \"Negative Tweet:\\n\" + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# making labels of the tweets\n",
    "labels_arr = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))\n",
    "labels = np.squeeze(labels_arr).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets needs to be preprocessed before going further. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "\n",
    "* Removing hyperlinks, hashtags and twitter markers\n",
    "* Tokenizing the string\n",
    "* Lowercasing\n",
    "* Removing stop words and punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess and tokenize a tweet\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Function to preprocess the tweets for sentiment analysis.\n",
    "    Input: \n",
    "        tweet - A string of tweet\n",
    "    Output: \n",
    "        tweet_clean - A list containing string stemmed words from the tweet\n",
    "    \"\"\"\n",
    "    ## STEP 1 - Remove hyperlinks, Twitter marks and styles\n",
    "    tweet_sub = re.sub(r'^RT[\\s]+', '', tweet)                  # remove old style retweet text \"RT\"\n",
    "    tweet_sub = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet_sub)  # remove hyperlinks\n",
    "    tweet_sub = re.sub(r'#', '', tweet_sub)                     # remove hashtags, only removing the hash # sign\n",
    "    tweet_sub = re.sub(r'\\$\\w*', '', tweet_sub)                 # remove stock market tickers like $GE\n",
    "   \n",
    "    ## STEP 2 - Tokenize the string and convert to lowercase and remove handles\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet_sub)\n",
    "    \n",
    "    ## STEP 3 - Remove stop words and punctuations\n",
    "    ## STEP 4 - Stemming\n",
    "    stopwords_english = stopwords.words('english') \n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    tweet_clean = []\n",
    "    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and   # remove stopwords\n",
    "            word not in string.punctuation):    # remove punctuation\n",
    "            stem_word = stemmer.stem(word)      # stemming word\n",
    "            tweet_clean.append(stem_word)\n",
    "            \n",
    "    return tweet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTweet:\n",
      "@KatCrisp1 Thanks for taking time to tweet this Kat :)\n",
      "\u001b[94mPreprocessed tweet:\n",
      "['thank', 'take', 'time', 'tweet', 'kat', ':)']\n",
      "\n",
      "\n",
      "\u001b[92mTweet:\n",
      "A sad new for the animal kingdom :( http://t.co/I7N9cinihz\n",
      "\u001b[94mPreprocessed tweet:\n",
      "['sad', 'new', 'anim', 'kingdom', ':(']\n"
     ]
    }
   ],
   "source": [
    "# testing the function preprocess_tweet\n",
    "tweet_num = random.randint(0,5000)\n",
    "print('\\033[92m' + \"Tweet:\\n\" + tweets[tweet_num])\n",
    "print('\\033[94m' + \"Preprocessed tweet:\")\n",
    "print(preprocess_tweet(tweets[tweet_num]))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "tweet_num = random.randint(5000,10000)\n",
    "print('\\033[92m' + \"Tweet:\\n\"+tweets[tweet_num])\n",
    "print('\\033[94m' + \"Preprocessed tweet:\")\n",
    "print(preprocess_tweet(tweets[tweet_num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a dictionary containing the word frequencies of all the words in the tokenized tweets and how much they occur is positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the freqency dictionary\n",
    "def build_freq_dict(tweets, labels):\n",
    "    \"\"\"\n",
    "    Function to build the frequency dictionary.\n",
    "    Input:\n",
    "        tweets - A list of all tweets\n",
    "        labels - A list of labels (positive/negative) for the tweets\n",
    "    Output:\n",
    "        freq_dict - A dictionary with (word, label) as key and its frequency as the value\n",
    "    \"\"\"\n",
    "    freq_dict = {}\n",
    "    \n",
    "    for tweet, y in zip(tweets, labels):\n",
    "        for word in preprocess_tweet(tweet):\n",
    "            key = (word,y)\n",
    "            freq_dict[key] = freq_dict.get(key, 0) + 1\n",
    "            \n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "# testing the build_freq_dict function\n",
    "fr = build_freq_dict(tweets, labels)\n",
    "print(fr[('happi',1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a vocabulary and then create a table which containes the word and its positive and negative frequencies for better vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the vocabulary\n",
    "vocab = []\n",
    "for tweet in tweets:\n",
    "        for word in preprocess_tweet(tweet):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "\n",
    "# building list representing table of word counts.\n",
    "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
    "freq_table = []\n",
    "for word in vocab:\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in fr:\n",
    "        pos = fr[(word, 1)]  \n",
    "    # retrieve number of negative counts\n",
    "    elif (word, 0) in fr:\n",
    "        neg = fr[(word, 0)]\n",
    "    # append the word counts to the table\n",
    "    freq_table.append([word, pos, neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['followfriday', 25, 0],\n",
       " ['top', 32, 0],\n",
       " ['engag', 7, 0],\n",
       " ['member', 16, 0],\n",
       " ['commun', 33, 0],\n",
       " ['week', 83, 0],\n",
       " [':)', 3568, 0],\n",
       " ['hey', 76, 0],\n",
       " ['jame', 7, 0],\n",
       " ['odd', 2, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_table[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for training and testing datasets\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array to list\n",
    "labels_train_y = np.squeeze(train_y).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build frequency dictionary just from the training data set\n",
    "freqs_dict = build_freq_dict(train_x, labels_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract features of a tweet using the frequency dictionary\n",
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # tokenize the tweet\n",
    "    words = preprocess_tweet(tweet)\n",
    "    \n",
    "    #initialize the feature vector and set the bias term to 1\n",
    "    x = np.zeros((1, 3)) \n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    for word in words:\n",
    "        x[0,1] += freqs.get((word,1.0), 0)      # increment the word count for the positive label 1\n",
    "        x[0,2] += freqs.get((word,0.0), 0)      # increment the word count for the negative label 0\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features of train_x\n",
    "train_x_features = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    train_x_features[i, :]= extract_features(train_x[i], freqs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features of test_x\n",
    "test_x_features = np.zeros((len(test_x), 3))\n",
    "for i in range(len(test_x)):\n",
    "    test_x_features[i, :]= extract_features(test_x[i], freqs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.020e+03, 6.100e+01],\n",
       "       [1.000e+00, 3.573e+03, 4.440e+02],\n",
       "       [1.000e+00, 3.005e+03, 1.150e+02],\n",
       "       ...,\n",
       "       [1.000e+00, 1.440e+02, 7.830e+02],\n",
       "       [1.000e+00, 2.050e+02, 3.890e+03],\n",
       "       [1.000e+00, 1.890e+02, 3.974e+03]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_features = np.squeeze(train_y)\n",
    "test_y_features = np.squeeze(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Logistic Regression Model\n",
    "LogReg = LogisticRegression(random_state=0)\n",
    "LogReg.fit(train_x_features,train_y_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the labels of the test tweets\n",
    "LogReg.predict(test_x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9915"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of prediction\n",
    "LogReg.score(test_x_features, test_y_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 99.15% from Logistic Regression. Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting my own tweet using the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'sunshin', 'outsid', 'today']\n",
      "[1.]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'i love the sunshine outside today @gunman'\n",
    "print(preprocess_tweet(my_tweet))\n",
    "\n",
    "x = extract_features(my_tweet, freqs_dict)\n",
    "\n",
    "y_predict = LogReg.predict(x)\n",
    "print(y_predict)\n",
    "\n",
    "if y_predict == 1:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following steps are required to implement a naive bayes classifier:\n",
    "\n",
    "#### $F_{pos}$ and $F_{neg}$\n",
    "- The positive and negative frequency of each word in the tweets using the frequency dictionary\n",
    "\n",
    "#### Calculate V\n",
    "- V is the number of unique words which appear in the frequency dictionary\n",
    "\n",
    "#### Calculate $N_{pos}$ and $N_{neg}$\n",
    "- Calculate the total number of positive words and total number of negative words\n",
    "\n",
    "#### Calculate logprior\n",
    "- Compute logprior using $log(D_{pos}) - log(D_{neg})$ where $D_{pos}$ is total number of positive tweets in the training dataset and $D_{neg}$ is total number of negative tweets in the training dataset\n",
    "\n",
    "#### Calculate the conditional probabilities p_w_pos and p_w_neg\n",
    "- Compute the conditional probabilities of each word in each category using the formula:\n",
    "$$ P(W/{pos}) = \\frac{F_{pos} + 1}{N_{pos} + V} $$\n",
    "$$ P(W/{neg}) = \\frac{F_{neg} + 1}{N_{neg} + V} $$\n",
    "\n",
    "#### Calculate log likelihood\n",
    "- Compute log likelihood using $log \\left( \\frac{P(W/{pos})}{P(W/{neg})} \\right)\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets\n",
    "    Output:\n",
    "        logprior: the log prior\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1] > 0:            # if the label is positive\n",
    "            N_pos += freqs[pair]\n",
    "        else:                      # else, the label is negative\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # number of total documents, positive documents and negative documents\n",
    "    D = len(train_y)\n",
    "    D_pos = np.sum(train_y)\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # logprior\n",
    "    logprior = np.log(D_pos/D_neg)\n",
    "\n",
    "    # loglikelihood\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        f_pos = freqs.get((word,1),0)\n",
    "        f_neg = freqs.get((word,0),0)\n",
    "\n",
    "        # conditional probabilities\n",
    "        p_w_pos = (f_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (f_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the naive bayes classifier using train_x\n",
    "logprior, loglikelihood = train_naive_bayes(freqs_dict, train_x, np.squeeze(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9089\n"
     ]
    }
   ],
   "source": [
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict with the Naive Bayes classifier using the logprior and loglikelihood with the below formula:\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: prediction\n",
    "\n",
    "    '''\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = preprocess_tweet(tweet)\n",
    "\n",
    "    # initialize probability to logprior\n",
    "    p = logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]     # add the log likelihood of that word to the probability\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the prediction p from the above function is positive, the tweet is predicted to have positive sentiment.\n",
    "Similarly, if the prediction p from the above function is negative, the tweet is predicted to have negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: accuracy of prediction\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:  # if the prediction is > 0\n",
    "            y_hat_i = 1                                              # the predicted class is 1\n",
    "        else:\n",
    "            y_hat_i = 0                                              # otherwise the predicted class is 0\n",
    "\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # Error\n",
    "    error = np.mean(np.abs(np.array([y_hats]) - test_y))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9940\n"
     ]
    }
   ],
   "source": [
    "# testing naive bayes classifier using test_x\n",
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, np.squeeze(test_y), logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 99.4% from Naive Bayes Classifier. Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting my own tweet using the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.801622640492191\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'you are bad :('\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)\n",
    "if p > 0:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
